{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9499e62e",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "In this notebook, <strong>word embeddings</strong> and <strong>sentence embeddings</strong> have been introduced. Word embeddings are an improvement over methods like <strong>Bag of Words (BoW)</strong> and <strong>TF-IDF</strong>, as they map words to vector spaces based on usage, though they still lack context sensitivity. \n",
    "\n",
    "To address this, <strong>contextual embeddings</strong> were developed with transformer models like <strong>BERT</strong> and <strong>RoBERTa</strong>. These models generate different embeddings for the same word based on its context, allowing for more accurate representations. This advancement has proven essential for tasks like <strong>question answering</strong> and <strong>word sense disambiguation</strong>, greatly enhancing natural language understanding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007af60",
   "metadata": {},
   "source": [
    "# Word embedding (Sementic embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df39994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk  # Natural Language Toolkit (NLP library for working with text data)\n",
    "from gensim.models import Word2Vec  # Gensim library for training Word2Vec model\n",
    "from nltk.corpus import stopwords  # NLTK's stopwords list to filter out common words (like 'the', 'is')\n",
    "import re  # Regular expression module for cleaning and processing text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9e9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple paragraph for the purpose of word embedding\n",
    "paragraph='''King sits on throne. Queen sits on throne. King is man. Quenn is woman. king is powerful. queen is powerful. man have moustache '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ved\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords from the NLTK corpus\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54e204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ', paragraph)  # Remove references or citations in square brackets (e.g., [1], [2])\n",
    "text = re.sub(r'\\s+', ' ', text)             # Replace multiple spaces with a single space\n",
    "text = text.lower()                          # Convert all text to lowercase to maintain consistency\n",
    "text = re.sub(r'\\d', ' ', text)              # Remove digits from the text (e.g., years or numbers)\n",
    "text = re.sub(r'\\s+', ' ', text)             # Again, replace multiple spaces with a single space after removing digits\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)         # Tokenize the text into sentences\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]  # Tokenize each sentence into words\n",
    "\n",
    "# Remove stopwords from each sentence\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]  # Filter out common stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7219cf38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'sits', 'throne', '.'],\n",
       " ['queen', 'sits', 'throne', '.'],\n",
       " ['king', 'man', '.'],\n",
       " ['quenn', 'woman', '.'],\n",
       " ['king', 'powerful', '.'],\n",
       " ['queen', 'powerful', '.'],\n",
       " ['man', 'moustache']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5cebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae4ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07380505 -0.01533471 -0.04536613  0.06554051 -0.0486016  -0.01816018\n",
      "  0.0287658   0.00991874 -0.08285215 -0.09448818]\n"
     ]
    }
   ],
   "source": [
    "# Finding Word Vectors\n",
    "vector = model.wv['king']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5902012d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 0.5436006188392639),\n",
       " ('powerful', 0.3293722867965698),\n",
       " ('moustache', 0.23243051767349243),\n",
       " ('woman', 0.035253241658210754),\n",
       " ('sits', -0.1799871027469635),\n",
       " ('man', -0.21132999658584595),\n",
       " ('throne', -0.38205233216285706),\n",
       " ('quenn', -0.5145737528800964),\n",
       " ('queen', -0.5381841063499451)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most similar words\n",
    "similar = model.wv.most_similar('king')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c83c3",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "    Now, let's check another example with Word Sense Disambiguation where samne word \"bank\" mean 2 different things.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cec0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='''The bank of river is awesome. The bank of india is awesome. The bank of america is awesome.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91780d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa68891f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bank', 'river', 'awesome', '.'],\n",
       " ['bank', 'india', 'awesome', '.'],\n",
       " ['bank', 'america', 'awesome', '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2238422f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', 'america', 'awesome', 'bank', 'india', 'river'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([item for sublist in sentences for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3dc0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([item for sublist in sentences for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d39034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d53890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'awesome', 'bank', 'america', 'india', 'river']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c124345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0, 'awesome': 1, 'bank': 2, 'america': 3, 'india': 4, 'river': 5}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39d545c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c47319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n",
      " -0.00946401  0.05768573 -0.07521638 -0.03936104]\n"
     ]
    }
   ],
   "source": [
    "# Finding Word Vectors\n",
    "vector = model.wv['bank']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef1078",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "So, in the above example of word embedding using <strong>Word2Vec</strong>, it can be seen that there is a single word vector for each word, irrespective of the context. For example, <strong>bank</strong> (financial institution) and <strong>bank</strong> (riverside) both have the same vector representation. And this is the problem.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd4307d",
   "metadata": {},
   "source": [
    "# Sentence Embedding (Contextual embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0865160",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872f0f0",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "    BERT, or Bidirectional Encoder Representations from Transformers, is a state-of-the-art language representation model developed by Google. It utilizes a bidirectional approach, allowing it to consider the context of a word based on both its left and right surroundings. This enables more nuanced understanding and improved performance in various natural language tasks.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63c1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ved\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    \n",
    "    # Pass through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the embeddings for the [CLS] token, which is used to represent the sentence\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    # Extract the word embeddings (all tokens except [CLS] and [SEP])\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze()  # Shape: (seq_length, hidden_size)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])  # Get tokenized words\n",
    "    \n",
    "    return sentence_embedding, word_embeddings, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6b4f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e20a81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9108641743659973\n",
      "0.8279022574424744\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The bank of river is awesome.\"\n",
    "sentence2 = \"The bank of india is awesome.\"\n",
    "sentence3 = \"The bank of america is awesome\"\n",
    "\n",
    "sentence_embedding1, word_embeddings1, tokens1  = get_sentence_embedding(sentence1)\n",
    "sentence_embedding2, word_embeddings2, tokens2 = get_sentence_embedding(sentence2)\n",
    "sentence_embedding3, word_embeddings3, tokens3 = get_sentence_embedding(sentence3)\n",
    "\n",
    "#similarity between sentence 1 and sentence 2\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding2))\n",
    "\n",
    "#similarity between sentence 1 and sentence 3\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4384c3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([9, 768])\n",
      "9\n",
      "['[CLS]', 'the', 'bank', 'of', 'river', 'is', 'awesome', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1.shape)\n",
    "print(word_embeddings1.shape)\n",
    "print(len(tokens1))\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aec6420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5993e-01, -2.8346e-01, -1.2725e-01,  6.7331e-02, -3.4368e-01,\n",
      "        -6.3199e-01,  2.2716e-01,  7.7732e-01,  2.0396e-01, -7.7004e-01,\n",
      "         2.6004e-01, -1.9773e-01,  2.0294e-01,  4.7289e-01,  1.6413e-01,\n",
      "         9.9682e-02,  2.1389e-03,  4.4647e-01,  6.2649e-01, -2.8840e-01,\n",
      "         1.9626e-01, -4.6361e-01, -3.3375e-02,  4.2548e-02,  2.0130e-01,\n",
      "        -2.1451e-01,  6.9227e-02,  2.7582e-01, -1.6637e-02,  2.0860e-01,\n",
      "         1.1602e-01,  4.4463e-01, -3.7375e-01, -4.0462e-01,  2.5003e-01,\n",
      "        -3.0714e-01,  2.8115e-01,  4.1827e-02, -1.5454e-01, -1.8375e-01,\n",
      "        -1.2003e-01,  4.3141e-02,  1.7394e-01,  5.4538e-02,  1.9144e-01,\n",
      "        -5.0717e-01, -3.3121e+00,  3.5465e-01, -1.5636e-01, -2.8077e-01,\n",
      "         3.7732e-01, -3.0141e-01, -1.6014e-01,  3.7137e-01,  1.6896e-01,\n",
      "         5.8219e-01, -6.8167e-01,  6.3972e-01,  3.3573e-01,  9.4391e-02,\n",
      "         2.4286e-01, -1.0007e-01,  7.1360e-02,  1.2184e-01,  7.9204e-03,\n",
      "         8.5833e-02,  4.3037e-02,  4.2669e-02,  1.2602e-01,  5.6273e-01,\n",
      "        -2.9512e-01, -1.3114e-01,  1.5114e-01, -5.3049e-01, -1.8102e-01,\n",
      "        -6.7368e-02, -4.0941e-01,  6.0052e-01, -2.1492e-01, -1.6247e-01,\n",
      "         2.8399e-01,  8.1302e-01,  1.0402e-01,  3.3496e-01,  8.9030e-02,\n",
      "         5.1316e-01, -2.1880e-01, -7.2698e-01, -2.0540e-01,  5.2097e-01,\n",
      "        -2.0092e-01,  9.2412e-02,  6.5207e-02,  3.1237e-01,  1.8806e-01,\n",
      "        -7.2926e-02, -7.2163e-02,  3.0990e-02,  1.8243e-01,  5.6269e-01,\n",
      "         3.8979e-01, -1.3152e-01, -5.0337e-02, -2.6203e-01,  1.3471e-01,\n",
      "        -2.6867e-01, -1.2464e-01, -1.5438e-01,  5.7114e-01, -2.3192e+00,\n",
      "         1.5597e-01, -9.3339e-02,  1.2076e-01, -3.8173e-02, -2.4497e-01,\n",
      "         5.6487e-01,  4.9293e-01,  3.3235e-01,  3.0844e-01,  1.5020e-01,\n",
      "        -5.8908e-03,  6.7443e-02,  2.0245e-01,  9.9925e-03,  1.1491e-01,\n",
      "         2.2578e-01,  7.3573e-02,  7.0828e-02, -1.6528e-01,  3.2354e-01,\n",
      "         2.8070e-01,  6.6516e-01,  8.1910e-02, -1.8029e-01,  1.8587e-01,\n",
      "        -2.9330e-01,  6.3915e-01, -2.9814e-01, -1.9997e-01, -4.6796e-02,\n",
      "        -6.0575e-01, -1.7441e-01, -2.7681e+00,  5.8367e-01,  6.0418e-01,\n",
      "        -1.4197e-01, -1.6585e-02, -2.7287e-01, -7.3864e-02, -1.6014e-01,\n",
      "         2.5932e-01,  2.9813e-01, -3.9656e-01,  1.7148e-01, -1.7716e-02,\n",
      "         6.2946e-02, -3.2764e-01,  6.8652e-02,  2.5065e-01,  7.3628e-01,\n",
      "         2.7934e-01, -8.3905e-02,  1.8896e-01, -1.8690e-01, -6.4570e-01,\n",
      "         1.1038e-01,  7.6077e-01,  7.4297e-01, -2.7485e-01,  7.1695e-03,\n",
      "        -1.5360e-01,  3.7522e-01, -1.9418e-02,  1.3639e-01,  3.9076e-01,\n",
      "        -3.2387e-01,  8.3726e-02,  4.1790e-01,  8.3091e-02,  8.4444e-02,\n",
      "        -4.1858e-01,  1.4922e-01, -1.9311e-02,  3.5824e-01,  5.6528e-01,\n",
      "        -1.8764e-01,  7.0920e-01,  4.6061e-01, -5.4756e-01, -8.0899e-02,\n",
      "         3.5535e-01, -5.7432e-01, -4.3584e-02,  5.8614e-02,  5.3183e-01,\n",
      "         8.3322e-02,  3.5294e-01, -6.4309e-01,  3.9750e-02,  2.1050e-01,\n",
      "        -3.5046e-02, -3.5979e-03, -9.1221e-02, -2.7877e-01, -5.5494e-01,\n",
      "         3.5608e+00,  3.6705e-01, -2.8442e-02, -1.2832e-01,  1.4476e-01,\n",
      "        -4.8256e-01,  6.4803e-02, -2.2230e-01, -2.5677e-01, -6.2461e-02,\n",
      "         2.0226e-01,  1.9278e-01,  1.5015e-01, -2.2181e-02, -1.6926e-01,\n",
      "        -3.1211e-02,  2.9907e-01, -5.3566e-01,  3.4756e-01, -3.7664e-01,\n",
      "        -5.9873e-01, -2.7603e-01,  2.6263e-01, -7.1012e-01, -1.4023e+00,\n",
      "        -3.3298e-01, -3.0467e-01,  2.3715e-02,  5.3421e-01, -1.5533e-01,\n",
      "         7.6514e-04,  8.3672e-02, -2.5230e-01,  5.1952e-01, -1.8441e-01,\n",
      "         1.9822e-01,  3.1802e-01, -2.7959e-02,  1.4072e-01,  2.7826e-02,\n",
      "         4.4967e-01,  1.8201e-01, -3.4087e-01,  5.7711e-02, -6.1825e-01,\n",
      "         2.6744e-01,  7.4057e-02,  5.6555e-01, -2.3449e-01,  2.6948e-02,\n",
      "        -1.8125e-01,  4.2883e-01, -1.0749e-01, -3.4034e-01, -8.5280e-02,\n",
      "        -1.2371e-01, -1.5965e-02,  1.3144e-01,  1.6632e-01, -7.7932e-01,\n",
      "        -3.8758e-01,  2.1985e-01,  4.1398e-02,  8.3388e-02, -4.0090e-01,\n",
      "        -9.0430e-02, -5.1360e-01, -2.3622e-01, -2.9351e+00,  3.7092e-01,\n",
      "         3.7115e-02,  3.0993e-01,  2.7624e-01, -3.5857e-01,  1.6273e-01,\n",
      "         4.1712e-01,  1.8667e-01, -4.1069e-01,  1.2319e-01,  1.3607e-02,\n",
      "        -9.7645e-02,  9.8995e-02, -2.2602e-01,  3.1464e-01, -2.4731e-01,\n",
      "         2.7417e-01, -3.9025e-01,  2.1908e-01,  3.1354e-02,  2.8268e-01,\n",
      "        -4.7544e-01,  2.9293e-01, -2.6305e-01, -5.3215e-02, -4.0240e-01,\n",
      "        -1.1707e-01,  5.6908e-01, -8.7889e-02, -2.0592e-01, -1.5975e-01,\n",
      "        -4.8279e-02,  2.6221e-01, -3.5010e-02, -3.3760e+00,  1.0067e-01,\n",
      "        -5.0122e-01, -4.0278e-01, -1.3823e-01, -8.6022e-02,  5.4486e-01,\n",
      "        -8.8370e-02, -8.1107e-01,  3.2064e-01,  2.7874e-01, -9.9869e-02,\n",
      "         5.3213e-02,  1.4060e-01,  4.9019e-01,  3.2439e-01,  2.6798e-01,\n",
      "        -1.7991e-01,  3.9318e-01,  4.9263e-02, -4.8502e-01, -7.2984e-02,\n",
      "         1.8049e-01, -2.5149e-01,  5.7251e-01,  3.9355e-01, -5.8032e-01,\n",
      "        -2.3408e-01, -4.1462e-01,  1.2626e-01, -2.0637e-02,  2.4921e-01,\n",
      "         2.7920e-01, -1.7141e-01, -3.7345e-01,  3.8993e-02,  5.3528e-01,\n",
      "        -1.4424e-02,  3.1486e-01,  3.8383e-02, -8.4728e-02,  3.2821e-01,\n",
      "        -2.2152e-01,  1.3368e-01,  4.8975e-01, -1.9203e-01, -2.9016e-01,\n",
      "         4.7429e-02,  1.9244e-01,  1.0745e-01,  1.1426e-01,  4.0897e-02,\n",
      "         9.2897e-01,  1.1881e-02, -3.8555e-01, -4.5113e-01,  4.4898e-01,\n",
      "         5.4552e-01, -4.5496e-02,  3.5539e-01,  6.2039e-01, -6.0031e-01,\n",
      "         4.7613e-01, -2.7335e-01,  2.1646e-01, -5.1470e-01,  4.0853e-01,\n",
      "        -3.4800e-01, -3.2310e-01,  1.2003e-01,  3.6071e-01,  1.2926e-01,\n",
      "        -3.4367e-01, -1.6156e+00,  2.5498e-01,  2.6567e-01, -5.5552e-01,\n",
      "         9.1800e-02,  2.9571e-01, -1.9339e-01, -2.5944e-01, -1.9267e-01,\n",
      "        -6.0307e-03,  8.4919e-01, -3.9295e-01, -3.3524e-01, -4.5911e-02,\n",
      "        -2.4286e-01, -6.6911e-01, -2.1172e-01,  1.0416e-01,  1.0147e-01,\n",
      "         2.8396e-01,  2.6655e-01, -2.5815e-01,  3.0814e-01,  2.6700e-01,\n",
      "        -9.1999e-01,  1.8052e-01, -5.2397e-01,  4.4023e-02, -1.4715e-01,\n",
      "        -7.1676e-02,  4.0974e-01, -1.6301e-02, -2.6831e-01,  1.2928e-01,\n",
      "         4.5086e-02,  1.2899e-01,  2.8026e-01, -5.1481e-01, -4.1457e-01,\n",
      "         3.3043e-01,  3.9974e-01,  4.6951e-01, -1.4555e-01,  2.2968e-01,\n",
      "         4.3611e-01, -1.8916e-01,  1.1867e-01,  4.3426e-01,  2.5018e-01,\n",
      "        -1.0998e-01, -5.4651e-02, -7.5998e-01, -1.6941e-01,  1.6994e-01,\n",
      "        -2.2143e-01, -6.4477e-01, -2.4253e-01,  1.6985e-01, -2.4869e-01,\n",
      "        -3.3937e-01, -4.1115e-01,  2.5936e-02, -2.0839e-01, -1.4440e-01,\n",
      "        -2.5178e-01,  4.7978e-01, -1.6841e-01,  5.3496e-01, -8.2339e-02,\n",
      "        -6.0197e-01,  1.9629e-01, -2.0503e-01,  7.4390e-01, -2.5922e-02,\n",
      "         2.6321e-01, -2.8875e-01,  4.8903e-01, -2.5102e-01, -1.1427e-01,\n",
      "        -4.2422e-01, -5.3896e-01,  2.3953e-01, -3.2788e-01, -1.1208e-01,\n",
      "         1.1121e-01,  1.0787e-01, -1.9955e-01,  2.6694e-01,  1.2140e-01,\n",
      "        -1.8738e+00,  4.5927e-01,  2.7005e-01,  3.2457e-02,  4.1280e-01,\n",
      "        -2.3812e-01, -5.6789e-01,  8.4827e-01,  1.1572e-01,  3.3621e-01,\n",
      "        -5.7577e-01,  9.4280e-02,  5.8064e-03,  1.4473e-01, -1.2203e-01,\n",
      "         3.1365e-01, -5.8319e-02, -3.0812e-01, -4.4010e-02, -3.1941e-01,\n",
      "         3.0961e-01,  1.5348e-01,  2.8884e-01, -1.8428e-03, -2.5857e-01,\n",
      "        -7.2958e-02, -1.5192e-01,  5.3844e-01,  1.6139e-01,  6.1863e-01,\n",
      "        -3.6418e-01, -2.7263e-01, -4.2075e-01, -5.0094e-01, -4.3592e-02,\n",
      "         4.3413e-01,  3.3016e-01, -1.8034e-01,  4.1186e-01,  4.2259e-01,\n",
      "        -1.5901e-01,  7.8201e-01,  1.1042e-01, -5.8944e-02,  6.7687e-01,\n",
      "         1.0499e+00, -9.3258e-02,  9.4894e-02,  4.8715e-02, -4.4492e-01,\n",
      "        -5.1732e-01, -4.4289e-01, -7.0057e-01,  1.3376e-01,  5.3555e-01,\n",
      "        -5.9435e-01,  1.7926e-01,  9.3863e-03, -7.4004e-01, -1.9709e-01,\n",
      "         3.7949e-01, -5.8802e-01, -8.6331e-01, -2.1267e-01, -4.9574e-03,\n",
      "        -3.8467e-01,  3.7122e-02, -5.7278e-01, -1.6011e-01,  1.8206e-01,\n",
      "         8.6609e-01, -1.5984e-02, -3.7663e-01,  2.6052e-01, -6.4295e-01,\n",
      "        -4.0236e-02,  2.9856e-01,  3.7801e-01, -1.1381e-02, -3.3907e-01,\n",
      "        -1.7385e-01,  5.4243e-03, -2.1274e-01,  3.5710e-01,  6.3258e-02,\n",
      "         3.9229e-01, -2.6279e-01, -1.2140e-01, -7.3448e-02, -1.2631e-01,\n",
      "        -2.7709e-01, -3.7966e-01,  5.4032e-01,  7.5873e-03, -5.4438e-02,\n",
      "         4.3057e-01,  3.8348e-01, -1.9012e-01, -2.4198e-01,  9.3174e-03,\n",
      "        -5.0784e-01,  7.9248e-02,  5.0829e-01,  1.4947e-02,  2.1503e-01,\n",
      "         2.8569e-01,  7.1366e-01, -1.0796e-01,  1.9818e-01, -4.4491e-01,\n",
      "         6.9229e-02, -7.2234e-02, -1.0145e-01, -4.8140e-01, -1.3852e-01,\n",
      "        -4.0920e-01,  5.0197e-01, -4.9367e-01,  1.9808e+00,  5.7646e-01,\n",
      "         1.5973e-01, -6.6427e-01,  3.1675e-01,  1.1230e-01,  1.2269e-01,\n",
      "         4.7811e-01, -7.6244e-02,  4.9950e-01, -6.1481e-01,  3.0721e-01,\n",
      "        -1.1414e-01, -1.3940e-01,  4.2287e-01, -1.9957e-01,  3.8657e-01,\n",
      "        -1.1316e-01, -5.8044e-01, -2.4175e-01, -7.3581e-01,  4.4327e-01,\n",
      "         1.4685e-01, -3.5433e-01,  5.0816e-02,  2.4066e-01,  5.1989e-01,\n",
      "         1.9177e-01,  2.9768e-01,  8.2768e-01, -4.3803e-01,  4.3148e-01,\n",
      "         6.7330e-02,  4.8593e-01, -6.5627e-01,  4.7015e-02, -1.7285e-01,\n",
      "        -1.3860e-01, -3.4313e-02, -2.9047e-01, -1.3103e-01, -7.6939e-01,\n",
      "         5.0833e-01,  3.3456e-01, -2.5463e-01,  6.4818e-01, -7.3636e-02,\n",
      "        -4.3058e-01,  6.4442e-01,  6.8815e-01,  3.5741e-01,  1.7117e-01,\n",
      "        -6.9802e-01, -2.2499e-01, -9.3655e-02, -2.9094e-01,  3.7999e-01,\n",
      "        -1.8801e-01, -2.6624e-01,  1.3924e-01,  1.2815e-01, -5.2497e-03,\n",
      "         2.5980e-01, -2.4924e-01, -2.9296e-02,  1.0709e-02,  1.4799e-01,\n",
      "         1.6947e-01, -8.6191e-02, -3.9609e-01, -1.2174e-01,  4.9941e-01,\n",
      "         5.5787e-01, -2.0246e-01,  6.1613e-01, -1.5315e-01,  3.4330e-01,\n",
      "        -2.6021e-01, -1.5603e-01, -2.5864e+00,  2.2603e-01,  6.9187e-01,\n",
      "        -8.6391e-02, -7.2952e-02,  3.7741e-01,  5.3387e-01, -2.8051e-02,\n",
      "         2.8830e-01, -1.6393e-01,  3.5383e-01,  5.0828e-01,  3.6315e-01,\n",
      "        -2.1226e-01,  2.3511e-01,  2.6021e-01,  6.5422e-01, -7.4097e-01,\n",
      "        -1.7295e-01, -9.2450e-02,  5.4833e-01,  2.5485e-01,  2.5320e-01,\n",
      "        -5.3357e-01, -6.2786e-01,  3.8306e-01, -1.3972e-01, -1.5502e-01,\n",
      "         3.0550e-01,  5.3664e-02, -1.2536e-01,  4.9979e-01,  1.4577e-03,\n",
      "        -2.9157e-01,  4.0411e-02, -1.8780e-01, -6.2569e-01,  1.6052e-01,\n",
      "         4.7933e-01,  2.0283e-01,  7.9166e-02,  7.8351e-01, -2.3863e-01,\n",
      "         1.6967e-01, -1.1474e-01,  5.4416e-02,  4.7658e-01, -1.5386e-01,\n",
      "         3.6020e-01, -2.9208e-01, -5.9655e-02, -7.5980e-02,  4.4890e-01,\n",
      "         2.6440e-02,  4.5882e-01,  2.9222e-01,  3.0855e-01,  9.6480e-02,\n",
      "         8.6248e-02, -1.9304e-01, -4.3500e-01, -1.1541e-01, -1.7228e-01,\n",
      "        -1.3579e-01,  3.1587e-01, -3.4968e-02, -6.8400e-02,  1.3290e-01,\n",
      "        -2.3410e-01, -2.7645e-01, -4.8444e-01, -2.4752e-01,  5.2451e-01,\n",
      "         1.9658e-01,  2.4090e-02, -2.1205e-01,  7.3745e-01, -8.9971e-02,\n",
      "         3.6549e-01,  9.1546e-02, -1.9150e-01,  2.2986e-02, -3.2093e-01,\n",
      "         1.3495e-01,  4.0420e-01, -6.7496e+00, -7.3068e-02, -2.0770e-01,\n",
      "        -3.1532e-01, -5.7068e-03, -7.6893e-01, -2.7168e-01, -1.8679e-01,\n",
      "        -3.4076e-02,  2.4283e-01, -2.5699e-02, -1.5608e-01, -6.8717e-02,\n",
      "        -7.3714e-01,  2.2050e-01,  4.1843e-01])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e7a60",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "It can be seen above that, sentence1 and sentence 2, are very similar because it is related to india. but, now, let's see the embessing of \"bank\" in sentence 1 and sentence 2.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fc097ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'bank', 'of', 'river', 'is', 'awesome', '.', '[SEP]']\n",
      "bank\n",
      "---------\n",
      "['[CLS]', 'the', 'bank', 'of', 'india', 'is', 'awesome', '.', '[SEP]']\n",
      "bank\n",
      "---------\n",
      "0.556409478187561\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)\n",
    "print(tokens1[2])\n",
    "print(\"---------\")\n",
    "print(tokens2)\n",
    "print(tokens2[2])\n",
    "\n",
    "print(\"---------\")\n",
    "print(cosine_similarity(word_embeddings1[2], word_embeddings2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555196d",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "    So, it can be seen that, though at sentence level sentence 1 and sentence 2 are similar but that similarity is not coming due to the word \"bank\", as \"bank\" in both sentences are having cosine similarity of 0.55 only.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bad7ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'bank', 'of', 'india', 'and', 'bank', 'of', 'bar', '##oda', 'situated', 'at', 'the', 'bank', 'of', 'gang', '##es', 'is', 'awesome', '[SEP]']\n",
      "bank\n",
      "bank\n",
      "bank\n",
      "0.9170936942100525\n",
      "0.6640843749046326\n",
      "0.6949294209480286\n"
     ]
    }
   ],
   "source": [
    "sentence5 = \"the bank of india and bank of baroda situated at the bank of ganges is awesome\"\n",
    "sentence_embedding5, word_embeddings5, tokens5 = get_sentence_embedding(sentence5)\n",
    "\n",
    "print(tokens5)\n",
    "print(tokens5[2])\n",
    "print(tokens5[6])\n",
    "print(tokens5[13])\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[6]))\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[13]))\n",
    "print(cosine_similarity(word_embeddings5[6], word_embeddings5[13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb00a75",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> In above sentence, there are three bank word, but it can be seen that, first 2 are reltaed to financial institution and third is riverside. And, hence, similarity score between first 2 bank is high but first and three or second and three is low. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00391e9",
   "metadata": {},
   "source": [
    "#### RoBERTa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be63141",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> RoBERTa, or A Robustly Optimized BERT Approach, is an advanced variant of BERT developed by Facebook AI. It enhances BERT's training methodology by utilizing larger batches and longer sequences while removing the next sentence prediction objective. This optimization results in improved performance across various natural language processing tasks. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9ec54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "def get_embeddings(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    \n",
    "    # Pass through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the sentence embedding from the <s> token (RoBERTa's equivalent of [CLS])\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "\n",
    "    # Extract the word embeddings (all tokens except <s> and </s>)\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze()  # Shape: (seq_length, hidden_size)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])  # Get tokenized words\n",
    "    \n",
    "    return sentence_embedding, word_embeddings, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8b74fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9983242750167847\n",
      "0.9942834377288818\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The bank of river is awesome.\"\n",
    "sentence2 = \"The bank of india is awesome.\"\n",
    "sentence3 = \"The bank of america awesome\"\n",
    "\n",
    "sentence_embedding1, word_embeddings1, tokens1  = get_embeddings(sentence1)\n",
    "sentence_embedding2, word_embeddings2, tokens2 = get_embeddings(sentence2)\n",
    "sentence_embedding3, word_embeddings3, tokens3 = get_embeddings(sentence3)\n",
    "\n",
    "#similarity between sentence 1 and sentence 2\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding2))\n",
    "\n",
    "#similarity between sentence 1 and sentence 3\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e47f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([9, 768])\n",
      "9\n",
      "['<s>', 'The', 'Ġbank', 'Ġof', 'Ġriver', 'Ġis', 'Ġawesome', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1.shape)\n",
    "print(word_embeddings1.shape)\n",
    "print(len(tokens1))\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388d3e4",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> The Ġ token in RoBERTa indicates a space before the word, helping the model differentiate between word boundaries and providing better context in text processing. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f883fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1294e-01,  1.2454e-01, -1.5777e-02, -1.2765e-01,  9.3037e-02,\n",
      "        -2.5061e-02, -4.4133e-02,  1.7352e-02,  5.5110e-02, -9.8595e-02,\n",
      "        -3.2031e-02,  2.1407e-02,  7.1863e-02, -3.3101e-02,  3.7476e-02,\n",
      "        -2.0152e-03, -1.2897e-02,  5.1805e-03, -8.5620e-03, -6.4059e-02,\n",
      "        -8.9887e-02,  2.0873e-02,  5.9324e-02,  1.2222e-01, -3.1684e-02,\n",
      "         7.9940e-02,  1.3939e-01,  6.8650e-02, -7.5467e-02,  3.2261e-02,\n",
      "        -4.9740e-02, -1.0487e-01,  5.5901e-02,  1.6350e-02,  4.5014e-03,\n",
      "         1.1428e-01,  1.4187e-02, -2.6811e-02, -3.9403e-02,  5.5864e-02,\n",
      "        -2.4190e-02,  1.6140e-01,  4.4250e-02, -2.8860e-02,  7.3279e-02,\n",
      "         2.6238e-02,  2.4181e-03, -1.8431e-02, -3.3616e-02, -2.6945e-02,\n",
      "        -9.5245e-03,  7.1390e-02, -5.8574e-02,  2.2873e-02, -1.3999e-01,\n",
      "         4.9996e-02,  3.6759e-02,  8.3483e-02,  4.5792e-02, -1.2199e-01,\n",
      "        -2.5181e-02, -1.8193e-01, -8.8339e-02, -7.0444e-02,  6.5770e-02,\n",
      "        -7.0507e-02, -1.3377e-02,  2.4619e-02,  2.0372e-02,  8.3795e-02,\n",
      "         7.5265e-02, -5.1153e-02,  2.1937e-02, -4.3599e-02,  2.6623e-02,\n",
      "        -5.0344e-04,  2.4117e-02,  5.8309e-01, -1.0157e-01, -7.6351e-03,\n",
      "         7.4904e-02, -7.1291e-02,  4.9000e-01, -1.4798e-03,  6.6717e-03,\n",
      "        -7.4751e-02,  1.3171e-01,  2.1710e-02,  1.8762e-02,  4.5160e-02,\n",
      "         2.8381e-03,  8.2181e-02, -4.9022e-02,  2.8552e-02,  4.2165e-02,\n",
      "         7.2386e-03, -2.4855e-02,  4.8533e-02, -9.1823e-02, -4.8817e-02,\n",
      "        -3.2566e-02, -7.6795e-02,  1.0887e-01,  1.3611e-01, -3.0740e-02,\n",
      "        -1.3239e-02,  6.2507e-02, -5.1148e-02,  4.7428e-02, -7.3861e-02,\n",
      "        -6.3814e-03, -1.9951e-02,  6.9611e-02, -8.0214e-03,  1.8022e-02,\n",
      "        -6.7458e-02,  1.3857e-03,  5.3633e-02,  4.8843e-02,  1.7390e-02,\n",
      "        -5.8142e-04,  8.0434e-02,  1.2430e-01, -5.7067e-02, -7.8035e-02,\n",
      "        -8.1551e-02, -8.4697e-02, -7.4533e-03, -3.0780e-02, -3.9308e-02,\n",
      "        -1.0756e-03, -1.6528e-01, -1.2443e-02,  7.5975e-02,  3.7631e-02,\n",
      "         3.4568e-02,  4.4692e-02, -5.8775e-02,  2.9387e-03, -6.7169e-02,\n",
      "        -1.3959e-03,  1.8520e-02,  4.7583e-02,  3.7431e-02,  1.0154e-01,\n",
      "         5.7054e-02, -1.3547e-02, -3.6016e-02,  3.5893e-04, -1.0642e-02,\n",
      "         6.5379e-02, -4.5494e-02, -5.4755e-02,  3.2536e-02, -5.6975e-02,\n",
      "         4.8952e-01,  9.8883e-02,  9.8165e-02, -4.3548e-02,  3.5935e-02,\n",
      "         1.8315e-01,  3.6008e-02,  1.5709e-02, -6.3139e-02, -5.5615e-02,\n",
      "        -3.7550e-02, -6.5774e-02, -4.3696e-04,  6.6189e-02, -7.5925e-03,\n",
      "         1.7106e-02,  4.5378e-02,  1.1282e-02, -7.2829e-02, -8.7185e-02,\n",
      "        -4.2431e-02,  6.4680e-02, -1.3862e-02, -1.1775e-01,  2.4110e-02,\n",
      "         2.4513e-02,  6.6647e-02, -9.4398e-02, -5.5913e-03, -5.0738e-02,\n",
      "         9.0434e-02,  1.4542e-02,  5.3788e-02, -3.5241e-02,  3.7421e-02,\n",
      "         6.8361e-02, -5.3944e-02,  1.3302e-03, -1.4007e-02, -3.1263e-02,\n",
      "         8.4213e-02, -4.5006e-02, -3.3648e-02,  1.2620e-02, -6.4335e-02,\n",
      "         1.2154e-02, -3.1651e-02,  1.2664e-01, -1.2607e-01,  2.7049e-02,\n",
      "        -2.7777e-02, -1.3826e-02,  5.3339e-02, -7.5880e-03, -1.0075e-01,\n",
      "        -2.9815e-02,  4.4568e-02, -2.1813e-02,  1.1229e-01,  7.7270e-02,\n",
      "        -2.5724e-02,  5.5009e-02,  2.1500e-01,  3.3357e-02,  4.1344e-02,\n",
      "         8.4960e-02,  1.0725e-02, -1.0030e-02,  9.0705e-02, -6.6840e-03,\n",
      "         2.4337e-02,  7.7261e-02, -4.4652e-03,  1.3750e-02, -5.0618e-02,\n",
      "        -5.3284e-02,  3.6782e-02,  4.6048e-02,  2.4203e-02,  7.7862e-02,\n",
      "        -1.4732e-01, -4.1167e-02, -8.0628e-03, -3.8165e-02,  7.4018e-02,\n",
      "        -5.1781e-02,  1.2445e-01,  8.5747e-02,  8.5396e-02, -6.4298e-03,\n",
      "         5.5909e-02, -1.0780e-03,  8.5478e-02, -4.1063e-03,  4.3333e-02,\n",
      "        -5.8181e-02, -6.6216e-02,  5.8137e-03,  1.3305e-02,  6.9129e-02,\n",
      "         7.3224e-03, -7.3831e-02, -2.7700e-02, -6.0183e-02,  1.3231e-02,\n",
      "        -9.0240e-02, -7.9855e-02,  3.6850e-02, -1.6242e-02, -9.9763e-02,\n",
      "        -4.4498e-02, -8.9615e-02, -1.8281e-02,  6.6134e-02, -3.8471e-02,\n",
      "        -2.3423e-03, -7.3910e-02, -3.4226e-02, -7.5878e-02,  1.3296e-02,\n",
      "        -2.7734e-02, -1.3536e-02,  4.9267e-03, -7.2087e-02,  1.4680e-02,\n",
      "         3.1843e-02,  7.4707e-03, -6.4189e-02, -2.3212e-02,  1.6242e-02,\n",
      "         1.8078e-02, -7.3350e-02,  2.6675e-02,  2.9592e-02,  5.8916e-02,\n",
      "         1.0027e-01,  4.2875e-02, -6.2590e-02,  4.7143e-02,  1.6820e-02,\n",
      "         6.3582e-03,  8.2452e-02, -1.4662e-02,  3.5423e-02, -1.3730e-02,\n",
      "        -4.7859e-02, -1.1081e-01, -5.7247e-02,  5.5513e-03, -1.4719e-02,\n",
      "        -6.8130e-02,  1.9418e-02, -5.0167e-02,  3.6360e-02,  2.4623e-02,\n",
      "        -3.8127e-02, -4.6984e-02, -1.3990e-01,  1.0502e-01,  3.8619e-02,\n",
      "         2.3836e-02,  1.0598e-01,  8.2170e-03,  2.0230e-02, -2.3283e-02,\n",
      "        -1.3526e-02,  3.5596e-02,  3.1898e-02, -2.9616e-02, -2.3911e-02,\n",
      "         8.2083e-02,  1.3580e-03,  9.0389e-03,  4.5706e-02,  3.5976e-01,\n",
      "        -4.3471e-01,  6.7417e-02,  8.7550e-02,  1.8340e-02,  8.1491e-02,\n",
      "         3.6158e-02,  5.0686e-02,  7.1764e-02,  1.1756e-01,  1.2141e-01,\n",
      "        -2.1121e-02, -5.0042e-03, -7.3879e-02,  5.7533e-02,  1.7938e-02,\n",
      "         1.0324e-03,  9.3819e-02, -3.6882e-02,  1.9318e-02,  1.8618e-02,\n",
      "        -5.7997e-02, -5.4444e-02, -1.7418e-02, -9.9496e-02,  2.6617e-02,\n",
      "         8.3241e-02,  3.5669e-02, -4.2116e-02,  3.0892e-02,  4.4390e-02,\n",
      "         2.2999e-02, -1.8581e-02, -4.5702e-03, -7.3247e-02,  5.2066e-02,\n",
      "        -4.0233e-02, -1.3821e-01,  1.1485e-01,  8.1145e-03,  4.7062e-02,\n",
      "         7.1998e-02, -9.6362e-02, -1.5136e-02, -1.2936e-02,  7.6444e-04,\n",
      "        -5.8531e-03,  3.3472e-02,  1.4928e-02,  1.3343e-02,  1.4742e-01,\n",
      "        -2.7701e-02,  5.2583e-02, -8.2990e-02,  5.6559e-02,  1.2846e-01,\n",
      "        -6.2861e-02,  8.8910e-02, -2.6561e-02,  7.6624e-02,  1.9377e-02,\n",
      "        -2.3935e-02, -6.8381e-02, -5.0829e-02, -1.1374e-01,  7.2760e-02,\n",
      "         1.2816e-02, -9.7639e-03, -1.3054e-01,  2.0889e-03, -5.2549e-02,\n",
      "         4.3429e-02,  9.3677e-03, -9.5908e-02,  4.4696e-02, -2.8136e-02,\n",
      "         2.6987e-03,  4.3742e-04, -4.1002e-02,  3.3333e-02, -6.4117e-02,\n",
      "        -1.2055e-02,  5.5410e-02,  1.3545e-01, -3.1414e-02,  2.6249e-02,\n",
      "        -4.6942e-02,  4.3615e-02, -1.2747e-02, -9.7089e-03, -2.5609e-02,\n",
      "        -1.0539e-01,  8.3252e-02,  1.0924e-02, -3.6318e-02, -6.4958e-03,\n",
      "         2.9699e-02, -2.9605e-02, -9.7434e-02, -6.0578e-02, -3.1615e-02,\n",
      "        -7.1429e-02,  5.0254e-02, -2.8981e-02, -1.2485e-02, -3.8913e-02,\n",
      "        -4.8554e-02,  3.8990e-02,  1.2669e-02, -2.3568e-02, -9.6046e-02,\n",
      "        -9.2889e-03, -8.1399e-02,  4.6878e-02,  8.1202e-02, -3.6748e-02,\n",
      "        -2.7781e-02,  4.9640e-02,  3.2513e-02, -3.4558e-03,  7.1403e-02,\n",
      "        -6.9038e-02, -2.3934e-02, -7.1455e-02, -4.2896e-01,  4.2116e-02,\n",
      "         8.9115e-03,  4.4956e-03,  4.6609e-02, -5.2955e-02,  3.3236e-02,\n",
      "         2.9241e-02,  5.3616e-03,  5.9106e-02, -5.1309e-02,  2.1827e-04,\n",
      "        -3.1000e-02, -7.4196e-02, -4.2385e-03, -2.5070e-02, -8.6491e-02,\n",
      "         8.2596e-02,  1.5412e-02,  4.0315e-03, -1.2239e-01,  5.0820e-02,\n",
      "        -1.5572e-02,  2.3469e-02,  3.9951e-04,  7.8190e-04, -1.2729e-02,\n",
      "        -8.3615e-02,  3.8856e-02,  5.3090e-02,  3.4022e-02, -9.1248e-02,\n",
      "        -4.6153e-02,  4.9738e-03,  4.4706e-02,  1.1265e-01,  1.4591e-02,\n",
      "        -5.3551e-02, -2.3075e-02,  9.1294e-02,  8.7436e-02,  2.0753e-01,\n",
      "        -3.4127e-02,  1.9795e-01,  1.9116e-02,  8.0501e-02,  6.1609e-02,\n",
      "        -2.6227e-02, -5.3166e-02, -2.4108e-02, -7.2750e-02, -2.1665e-02,\n",
      "         6.6410e-02, -4.2350e-02, -9.5594e-02,  3.3535e-02,  4.0638e-02,\n",
      "        -2.5152e-02,  2.5406e-02,  1.9810e-01,  1.7180e-02,  3.4946e-02,\n",
      "        -2.1628e-02, -3.1982e-02,  2.5701e-02, -4.3033e-02,  1.8019e-02,\n",
      "        -3.5568e-02, -3.3717e-02, -4.1965e-02,  1.0908e-02, -8.9368e-02,\n",
      "        -2.7174e-02,  1.4552e-02,  4.2700e-03,  1.1124e-01, -1.2302e-02,\n",
      "         6.2447e-02, -4.7178e-02,  8.9596e-02,  3.7733e-02, -4.3181e-03,\n",
      "         3.8335e-02,  1.2961e-01,  1.2416e-02, -5.0298e-02, -3.1337e-02,\n",
      "        -1.8581e-02, -3.9042e-03,  7.0130e-02, -3.9360e-02,  1.4114e-01,\n",
      "         1.1690e-01, -2.8987e-03, -6.2442e-02,  5.7153e-02,  1.8635e-02,\n",
      "         3.7526e-02, -6.3086e-01, -7.9943e-02,  1.0576e-01, -2.6789e-04,\n",
      "         1.2227e-02,  2.5907e-02,  7.5601e-02, -4.8968e-02, -1.3225e-02,\n",
      "        -2.8562e-02,  8.8895e-02,  3.7825e-02,  9.1858e-02, -5.2071e-02,\n",
      "        -3.8147e-02,  5.7552e-02, -1.3915e-02,  2.1522e-02,  8.2053e-02,\n",
      "        -3.0379e-01, -2.5434e-02, -8.4523e-02,  1.2672e-01, -3.6739e-02,\n",
      "         8.8859e-02,  1.0683e-01, -7.2001e-02,  5.3253e-02,  3.6775e-02,\n",
      "         2.2851e-02,  2.7665e-02,  1.0437e-02, -7.1560e-02,  4.6515e-02,\n",
      "         9.7906e-02,  3.2227e-02,  4.4940e-02,  1.1852e+01,  2.5709e-02,\n",
      "         4.8192e-02, -4.8362e-02,  6.8017e-02, -7.9244e-02,  7.2728e-03,\n",
      "        -1.4103e-01, -3.7984e-02,  1.4262e-01, -4.0903e-02, -5.4563e-02,\n",
      "        -3.5519e-02, -1.0118e-01,  4.3243e-02,  1.6467e-03, -7.8028e-02,\n",
      "        -4.7015e-02,  3.7989e-02, -7.6050e-02, -4.3802e-02,  2.1532e-02,\n",
      "         4.9349e-02, -1.8525e-02, -7.1332e-02,  6.1027e-02,  7.4499e-02,\n",
      "        -6.7868e-02, -3.9571e-02,  2.8729e-02, -8.0317e-03,  7.7113e-02,\n",
      "         3.5592e-02, -2.5531e-02,  1.2983e-01, -7.2887e-03,  3.2549e-02,\n",
      "         7.8780e-02, -6.7072e-03,  4.1082e-02,  1.9471e-02, -4.1860e-04,\n",
      "         8.3252e-02,  2.7304e-02,  7.8856e-02,  5.0163e-03,  1.8999e-02,\n",
      "         1.0386e-01, -1.0667e-02,  5.1284e-02,  1.4560e-01, -7.9493e-02,\n",
      "         1.1159e-01,  4.7634e-02, -3.1866e-03,  4.2191e-02,  6.4340e-02,\n",
      "        -5.4320e-03,  6.9261e-02,  2.8565e-02, -7.2930e-02,  5.5246e-02,\n",
      "         4.7186e-03,  6.3864e-02, -1.9596e-02,  1.2461e-01,  7.3630e-02,\n",
      "         9.9883e-02, -1.0655e-01,  6.9625e-03, -8.8883e-03, -8.2243e-02,\n",
      "        -6.8321e-02, -1.4533e-02,  5.9962e-02, -2.3776e-02, -1.6063e-02,\n",
      "        -1.3290e-02, -4.0224e-03, -3.9081e-02,  4.8752e-02,  5.5929e-02,\n",
      "         3.6303e-02, -3.2876e-02,  1.2886e-01,  1.6441e-02,  7.1467e-02,\n",
      "         4.8035e-02, -3.7052e-02, -1.0639e-01, -3.1240e-03,  5.9072e-02,\n",
      "        -4.0211e-02, -4.7835e-02,  1.0780e-02, -1.3887e-02,  4.7618e-02,\n",
      "        -1.4626e-01,  2.9894e-02,  4.8807e-02, -1.1152e-01,  1.5430e-02,\n",
      "        -2.2819e-02,  5.8442e-02, -4.6726e-03,  2.3461e-02, -4.0350e-02,\n",
      "        -4.5415e-02,  3.0564e-02, -1.5481e-02,  2.0403e-02,  1.2818e-03,\n",
      "         4.9920e-02, -6.7945e-02, -4.1646e-03,  5.5699e-02, -7.2801e-03,\n",
      "         1.7485e-02,  4.1032e-02,  8.8915e-02, -1.0651e-01,  1.4007e-02,\n",
      "        -4.8294e-02, -1.1867e-02, -4.7235e-02, -8.3599e-02, -2.3168e-02,\n",
      "         6.1720e-02, -1.1385e-02,  8.8321e-03, -2.6670e-02,  3.2577e-02,\n",
      "         1.6369e-02, -5.4692e-02,  7.1521e-02, -3.7792e-02, -1.8789e-02,\n",
      "         6.1743e-02,  5.1409e-02,  8.0185e-02, -2.6393e-02, -6.4785e-02,\n",
      "        -6.1956e-02, -7.8385e-02,  6.5917e-03,  7.7035e-02,  5.0106e-02,\n",
      "         4.2878e-02,  1.1769e-01, -8.9334e-02, -4.3927e-02,  3.9743e-02,\n",
      "         1.2820e-01,  2.0926e-02,  1.1375e-03, -3.7669e-02, -3.8106e-02,\n",
      "         1.2087e-01,  2.0144e-03,  1.7827e-03,  4.1832e-02,  5.1365e-02,\n",
      "         6.4545e-02,  6.8408e-02,  2.0478e-02,  3.8315e-02, -4.9268e-02,\n",
      "         4.7770e-02,  4.8958e-02, -3.0608e-02,  6.4894e-04,  2.8540e-02,\n",
      "        -1.5010e-01, -1.1890e-01,  2.1138e-02,  1.4475e-01,  5.2503e-02,\n",
      "        -1.0769e-01, -2.2279e-02, -5.0191e-02])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f27a7395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'Ġbank', 'Ġof', 'Ġriver', 'Ġis', 'Ġawesome', '.', '</s>']\n",
      "Ġbank\n",
      "---------\n",
      "['<s>', 'The', 'Ġbank', 'Ġof', 'Ġind', 'ia', 'Ġis', 'Ġawesome', '.', '</s>']\n",
      "Ġbank\n",
      "---------\n",
      "0.9483951926231384\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)\n",
    "print(tokens1[2])\n",
    "print(\"---------\")\n",
    "print(tokens2)\n",
    "print(tokens2[2])\n",
    "\n",
    "print(\"---------\")\n",
    "print(cosine_similarity(word_embeddings1[2], word_embeddings2[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "918708b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', 'Ġbank', 'Ġof', 'Ġind', 'ia', 'Ġand', 'Ġbank', 'Ġof', 'Ġbar', 'oda', 'Ġsituated', 'Ġat', 'Ġthe', 'Ġbank', 'Ġof', 'Ġg', 'anges', 'Ġis', 'Ġawesome', '</s>']\n",
      "Ġbank\n",
      "Ġand\n",
      "Ġthe\n",
      "0.9874536395072937\n",
      "0.9697230458259583\n",
      "0.9586929082870483\n"
     ]
    }
   ],
   "source": [
    "sentence5 = \"the bank of india and bank of baroda situated at the bank of ganges is awesome\"\n",
    "sentence_embedding5, word_embeddings5, tokens5 = get_sentence_embedding(sentence5)\n",
    "\n",
    "print(tokens5)\n",
    "print(tokens5[2])\n",
    "print(tokens5[6])\n",
    "print(tokens5[13])\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[7]))\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[14]))\n",
    "print(cosine_similarity(word_embeddings5[7], word_embeddings5[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfdf3b",
   "metadata": {},
   "source": [
    "####  GPT-2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832323d5",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> GPT-2, or Generative Pre-trained Transformer 2, is a language model developed by OpenAI. It employs an autoregressive approach, predicting the next word in a sentence based on preceding context. With its extensive training on diverse internet text, GPT-2 excels in generating coherent and contextually relevant text across various applications. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5e92dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "def get_embeddings(sentence):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    \n",
    "    # Pass through the model to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # GPT-2 doesn't have a [CLS] token, so we average the word embeddings to get the sentence embedding\n",
    "    word_embeddings = outputs.last_hidden_state.squeeze()  # Shape: (seq_length, hidden_size)\n",
    "    \n",
    "    # Sentence embedding can be derived by averaging word embeddings\n",
    "    sentence_embedding = word_embeddings.mean(dim=0)\n",
    "    \n",
    "    # Get the tokens (GPT-2 uses byte-pair encoding, so tokens may differ)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    return sentence_embedding, word_embeddings, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c14b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995173811912537\n",
      "0.9984961152076721\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The bank of river is awesome.\"\n",
    "sentence2 = \"The bank of india is awesome.\"\n",
    "sentence3 = \"The bank of america awesome\"\n",
    "\n",
    "sentence_embedding1, word_embeddings1, tokens1  = get_embeddings(sentence1)\n",
    "sentence_embedding2, word_embeddings2, tokens2 = get_embeddings(sentence2)\n",
    "sentence_embedding3, word_embeddings3, tokens3 = get_embeddings(sentence3)\n",
    "\n",
    "#similarity between sentence 1 and sentence 2\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding2))\n",
    "\n",
    "#similarity between sentence 1 and sentence 3\n",
    "print(cosine_similarity(sentence_embedding1, sentence_embedding3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef82ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([7, 768])\n",
      "7\n",
      "['The', 'Ġbank', 'Ġof', 'Ġriver', 'Ġis', 'Ġawesome', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1.shape)\n",
    "print(word_embeddings1.shape)\n",
    "print(len(tokens1))\n",
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5464ab",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> In GPT-2, the Ġ token signifies a space before the subsequent word, aiding in recognizing word boundaries and enhancing the model's understanding of text structure.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6549e099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.2720e-02,  1.2177e-02, -6.6039e-01,  1.3097e-01,  2.0369e-01,\n",
      "        -2.0988e-02,  4.3167e+00,  2.6947e-01, -2.6785e-02, -1.7381e-01,\n",
      "        -5.9662e-02, -4.0658e-02,  3.5592e-02,  4.6588e-02,  4.3792e-02,\n",
      "        -2.7996e-01, -1.7147e-01, -6.0552e-02, -3.0818e-01, -6.7432e-01,\n",
      "         1.2491e-01,  9.0983e-02, -2.2381e-01,  3.9666e-02,  8.6331e-02,\n",
      "         2.8157e-01, -5.2173e-02, -1.2894e-01, -2.4060e-01, -3.8236e-02,\n",
      "        -6.3042e-02, -1.0419e-01,  7.8191e-02, -3.7931e-02, -1.3611e-01,\n",
      "         4.3478e-01,  6.2503e+01,  2.4032e-01, -2.8039e-01,  2.9788e-01,\n",
      "         2.5196e-01,  2.3417e-01,  1.1968e-01, -5.2832e-02, -7.9778e-02,\n",
      "        -5.0708e-02, -9.2995e-02, -3.8076e-01, -1.6937e-01,  1.4483e-01,\n",
      "        -4.3260e-02,  1.4739e-01,  3.1843e-01,  7.7430e-03, -2.4752e-01,\n",
      "         6.5987e-01,  6.1598e-01, -1.3300e-01,  1.0418e-01, -2.5831e-01,\n",
      "        -5.8405e-02, -1.7147e-01,  3.9393e-02,  2.6453e-01, -1.0963e+00,\n",
      "         8.3859e-02, -1.1631e-01, -1.0500e-01, -5.1724e-01, -2.5389e-01,\n",
      "         6.3768e-03, -3.7282e-02, -1.5420e-01, -1.3485e-01, -1.6555e-01,\n",
      "         4.5409e-03, -1.0203e-01,  9.1579e-04, -3.0016e-01, -1.0922e-01,\n",
      "        -3.4793e-01, -5.6607e-02,  2.2079e-01, -4.4136e-01, -6.2604e-02,\n",
      "        -6.5806e-02, -1.3395e-01, -1.0198e+00,  1.3077e-01, -1.0484e-02,\n",
      "         5.1608e-02,  9.7067e-02, -2.6391e-01,  2.7856e-02, -2.0506e-01,\n",
      "        -1.3710e-01, -1.3729e-01, -4.3234e-01, -1.4823e-01,  5.2540e-01,\n",
      "         4.3730e-02,  4.8557e-02,  3.2706e-01, -2.2299e-02,  1.4243e-01,\n",
      "        -3.0134e-01, -1.9995e-01,  8.0106e-01, -6.9837e-02, -2.8384e-01,\n",
      "         3.3717e-02, -4.2720e-02, -1.4786e-01,  2.6310e-01,  1.5335e-01,\n",
      "         4.7356e-02, -1.2872e-01, -2.1152e-01, -3.1424e-01, -1.9972e-01,\n",
      "         1.9820e-02,  6.6205e-02,  1.5990e-01,  7.7022e-02, -1.6489e-01,\n",
      "        -8.0084e-03, -8.6735e-02,  2.2708e-01, -3.8097e-01,  2.1955e-01,\n",
      "         1.0526e-03, -2.7564e-01,  2.2023e-01,  5.5902e-02, -5.7807e-02,\n",
      "         4.6959e-01, -3.9857e-02, -1.4623e-01,  5.3944e-01, -2.0074e-02,\n",
      "        -6.5236e-02,  6.3169e-02, -2.7604e-01, -5.1804e-02,  1.2058e-01,\n",
      "         3.7872e-01,  2.2117e-01,  1.5768e-01,  3.7088e-02,  1.3790e-01,\n",
      "        -1.1929e-02, -7.1845e-02,  6.7488e-02, -1.1410e-01, -3.8856e-02,\n",
      "        -4.8581e-02,  8.0294e-02, -1.4118e-03, -1.9784e-01, -4.9546e-02,\n",
      "         2.7235e-01, -1.7816e-01, -1.9876e-01, -1.9428e-01, -8.8971e-02,\n",
      "         2.0347e-01,  2.1289e-02, -9.2454e-01, -1.5431e-01,  1.9299e-01,\n",
      "        -1.4949e-01, -1.5733e-01, -1.1891e-01, -4.2968e-02, -1.6555e-02,\n",
      "        -3.4314e-01, -4.5526e-01,  1.5333e-01,  2.5967e-02, -1.9223e-03,\n",
      "         5.7918e-02,  6.8310e-02,  2.4577e-01, -3.0000e-01, -1.0283e-01,\n",
      "        -2.1451e-01,  2.1061e-01,  1.6830e-02, -7.8691e-02, -1.5473e-01,\n",
      "        -1.3945e-01, -3.8169e-01, -1.2912e-01,  2.7731e-02,  2.0037e-02,\n",
      "        -6.4646e-01,  2.1725e-01,  1.3879e-02, -9.4794e-02, -2.9766e-01,\n",
      "        -2.0642e-02,  1.6690e-01,  2.2189e-02, -1.4656e-01, -2.1061e-02,\n",
      "        -1.0373e-01, -1.3921e-01, -1.0268e-01, -1.3101e-01,  1.6497e-01,\n",
      "         9.5706e-02,  1.1172e-01,  1.8502e-03, -2.8045e-01, -9.7353e-02,\n",
      "         5.3994e-02, -1.0991e-01, -3.2106e-01, -1.0103e-01, -9.5575e-02,\n",
      "         2.4565e-01,  4.6590e-02,  2.3789e-01, -2.2654e-01, -6.5786e-02,\n",
      "        -1.5641e-01, -2.5694e-01, -1.4513e-01,  3.6709e-03, -5.1324e-02,\n",
      "         6.5110e-02, -9.8715e-02,  1.0760e+00, -7.7499e-02,  4.1258e-03,\n",
      "        -3.1734e-02, -1.0651e-01, -1.6512e-01,  3.4808e-02, -4.5566e-02,\n",
      "         3.8587e-01,  1.5117e-01, -8.6181e-02,  5.0576e-02,  2.3710e-02,\n",
      "         1.1851e-01, -3.1462e-01,  2.8531e-01,  1.6643e-02,  2.1495e-01,\n",
      "        -3.9696e-01, -4.9895e-02,  1.3377e-01, -5.1860e-02, -2.7297e-01,\n",
      "         3.5771e-01, -1.0313e-01,  7.1319e-02,  1.2546e-01, -9.0086e-02,\n",
      "        -2.9738e-01,  4.6413e-01, -6.0303e-02, -1.6268e-02, -8.4140e-02,\n",
      "         4.4024e-02, -1.0063e+00,  3.1856e-01,  1.9989e-01,  3.0605e-01,\n",
      "         5.5900e-02,  5.8988e-01,  3.0900e-01, -1.9216e-01, -8.2973e-02,\n",
      "        -2.3676e-02, -3.8605e-02, -1.6981e-02,  3.1752e-02,  9.8510e-02,\n",
      "         9.3843e-02, -2.3653e-01, -2.7680e-01, -5.8033e-03, -7.9991e-02,\n",
      "         1.6677e-02,  1.0835e-02, -9.9591e-02,  5.5040e-01,  9.4699e-02,\n",
      "        -1.6090e-01, -2.2959e-01,  2.0387e-01,  1.3066e-01, -7.9097e-02,\n",
      "         1.0447e-01, -1.6299e-02, -4.8788e-02,  6.7346e-02,  6.0083e-02,\n",
      "        -3.2133e-02,  1.1269e-01,  2.7047e-03, -6.3216e-02,  1.3162e-01,\n",
      "        -6.9560e-02,  8.8605e-02, -3.0549e-02, -7.3268e-01,  8.8300e-02,\n",
      "        -7.0776e-02, -1.0886e-01,  1.4077e-01, -2.1172e-01, -2.4419e+01,\n",
      "        -4.1101e-01, -1.7620e-01,  7.3335e-02, -2.5611e-02,  2.3009e-02,\n",
      "        -3.1914e-01, -2.1138e-01,  1.6030e-01,  5.3804e-02, -2.7653e-01,\n",
      "        -1.1778e-01,  1.2901e-01, -1.0126e-01, -6.8482e-02, -1.2428e-01,\n",
      "         5.9602e-02, -1.8473e-01, -1.3311e-01,  1.4468e-01, -1.6263e-01,\n",
      "        -6.2697e-03, -1.9256e-01, -1.4749e-02, -4.6450e-02,  2.9755e-02,\n",
      "        -9.2618e-03,  3.0954e-02, -3.0894e-02, -2.1695e-01,  8.4546e-02,\n",
      "         1.2940e-01, -8.8802e-02,  2.7886e-01,  6.0265e-02, -1.0522e-01,\n",
      "        -2.2341e-01,  4.6530e-02,  2.4056e-01, -1.9697e-02,  9.1284e-02,\n",
      "        -2.2007e-01, -3.6959e-01,  1.3442e-01,  8.0950e-02, -4.3540e-02,\n",
      "         1.4063e-01,  6.6508e-01,  1.3210e+00, -1.4276e-02, -5.3806e-02,\n",
      "        -1.1906e+00, -1.5560e-01, -2.2687e-04, -1.1562e-01, -1.0827e-01,\n",
      "         9.5480e-02,  2.3274e-01, -1.0998e-01, -1.1395e+00, -1.4245e+01,\n",
      "        -1.5625e-01,  9.6536e-03,  5.2703e-01, -2.8455e-02,  2.4320e-01,\n",
      "         1.9445e-01,  6.8425e-02,  6.1484e-02, -3.0650e-01, -8.9426e-02,\n",
      "         8.5123e-02, -1.9072e-01, -4.5706e-03,  1.4660e-01,  1.7287e-01,\n",
      "         3.6314e-02,  1.6175e-01, -3.6490e-02, -5.5991e-02, -9.5718e-02,\n",
      "         2.0449e-01, -5.7872e-01,  1.4376e-01,  1.3864e-01,  4.5378e-02,\n",
      "         1.9445e-01, -1.7256e-01, -5.0106e-01, -7.1751e-02, -4.8840e-02,\n",
      "        -4.0016e-02,  3.8080e-02,  1.8968e-01,  3.0603e+00, -9.4348e-02,\n",
      "        -1.7349e-01,  7.3296e-02, -1.4366e-01, -2.1139e-01, -1.1237e-01,\n",
      "        -4.2193e-01, -9.8728e-02, -3.8940e-01, -1.8937e-01,  2.3437e-01,\n",
      "         1.2083e-01, -1.2677e-01,  5.0920e-02, -2.1519e-02, -8.1944e-02,\n",
      "        -1.8715e-01, -1.1352e-01,  2.3879e-02,  7.9900e-02, -1.3180e-01,\n",
      "         7.5038e+01, -2.2460e-01,  8.0876e-02, -2.1227e-01, -1.3067e-01,\n",
      "        -5.8396e-02,  1.0664e-01,  1.3597e-02,  3.6507e-01,  1.8249e-01,\n",
      "         3.4815e-02, -9.0425e-02,  3.6390e+00,  2.7640e-02, -9.2391e-02,\n",
      "        -7.2452e-02, -3.0253e-01,  1.1843e+00, -8.6629e-02, -2.9516e-02,\n",
      "        -4.1390e-01,  2.3264e-01, -8.6635e-02, -1.8116e-03,  1.8847e-01,\n",
      "        -1.6439e-01, -2.4291e-01, -1.7539e-01, -6.9659e-02,  5.9613e-02,\n",
      "         1.9645e-01,  6.7911e-03, -1.2902e-01, -4.9555e-02,  1.2391e-01,\n",
      "        -2.8327e-01,  2.2378e-01, -2.7629e-02, -3.2351e-02,  9.5409e-02,\n",
      "         7.1217e-02,  3.3575e-02, -3.8785e-01,  6.4319e-02,  1.3061e-01,\n",
      "         4.1756e-02,  3.6067e-02, -5.4961e-02, -1.7930e-01, -1.1352e+00,\n",
      "        -8.8860e-01,  1.1407e+00,  1.0972e-01, -5.5809e-03,  7.0079e-02,\n",
      "         1.4379e-01,  8.1602e-02, -1.6285e-01,  3.7147e-01, -3.6908e-01,\n",
      "        -6.6006e-02, -9.7389e-03, -1.8913e-01, -3.0677e-01, -2.1972e-01,\n",
      "         2.4715e-02,  1.7719e+02, -4.9206e-01,  6.7000e-02,  1.6222e-01,\n",
      "         2.4722e-01, -1.7253e-01, -3.9937e-02, -9.4881e-01, -6.9324e-02,\n",
      "        -2.8468e-01,  9.7768e-03,  4.6107e-01,  2.9444e-02,  8.2714e-05,\n",
      "         1.3896e-01, -1.9906e-01,  3.8347e-03, -3.3888e-01, -2.0995e-01,\n",
      "        -1.9073e-01, -2.2562e-01, -1.4843e-02,  2.7787e-01, -2.4415e-01,\n",
      "         2.3420e-02,  4.0022e-02,  8.5298e-02,  1.7391e-01, -3.8463e-01,\n",
      "        -1.7601e-01, -4.7006e-02, -5.7689e-01, -2.4585e-01,  2.3628e-01,\n",
      "        -1.7455e-02,  5.9306e-02,  9.7685e-02,  5.3425e-02, -4.6456e-02,\n",
      "         3.9398e-01, -3.6356e-02,  1.6574e-02, -1.4911e-01,  2.5590e-04,\n",
      "         4.4005e-01, -2.7617e-01, -1.1489e-01,  3.3650e-01, -4.9558e-02,\n",
      "         4.1496e-03, -7.9275e-02, -1.5721e-01,  1.5437e-02, -4.8530e-03,\n",
      "         1.7032e-01,  5.2404e-01,  2.2918e-02, -1.0967e-01, -5.1140e-01,\n",
      "        -4.2965e-03, -1.2549e-01, -4.9168e-02, -7.4326e-02,  4.8310e-03,\n",
      "        -3.6552e-01,  1.5800e-01,  1.5220e-02,  1.7170e-02,  4.2014e-01,\n",
      "        -2.5262e-01,  3.8713e-01, -2.3429e-01, -3.8635e-01, -1.6552e-01,\n",
      "         2.0912e-01,  1.8961e-01, -8.5313e-03,  1.0988e-02,  5.1113e-02,\n",
      "        -2.0065e-01,  4.1697e-02,  1.3975e-01, -1.7003e-01,  1.6237e-01,\n",
      "         4.6488e-02, -7.1470e-02, -5.5679e-01, -1.3742e-01,  6.6676e-02,\n",
      "         2.4423e-03,  3.0885e-01, -2.3720e-01,  3.5562e-01, -1.7239e-01,\n",
      "         1.7363e-01,  1.0132e-01,  1.6163e-01,  1.0357e-01,  4.7514e-04,\n",
      "        -1.6545e-01, -1.4094e-02, -2.3657e-01, -1.9767e-02, -9.3275e-02,\n",
      "        -2.0014e-01, -3.7400e-01, -1.6512e-01, -1.4989e-01,  2.4980e-01,\n",
      "         1.0544e-01, -2.0197e-01,  1.4261e-01,  4.5720e-01, -2.5212e-01,\n",
      "        -1.5606e-02,  1.5274e-01,  3.1505e-02, -3.0836e-01,  5.5265e-02,\n",
      "        -1.2670e-01,  1.1704e-01,  4.6334e-02,  1.3787e-02,  6.4380e-02,\n",
      "        -8.5118e-02,  2.4354e-01,  2.2060e-01,  3.9261e-01, -6.9714e-02,\n",
      "        -1.8055e-01, -7.8347e-02, -4.0012e-01, -5.5430e-02, -1.0937e-01,\n",
      "         3.9295e-01, -3.2033e-01,  8.4018e-02, -3.6447e-02,  2.4130e-01,\n",
      "        -2.4943e-01, -2.4372e-01, -1.0845e-01, -1.1614e-01,  1.0249e-02,\n",
      "         3.0408e-01, -1.0191e-01,  4.6484e-03, -1.0971e-01,  3.1406e-01,\n",
      "        -2.0564e-01, -1.3421e-01,  8.4762e-02,  5.8240e-02, -4.3969e-01,\n",
      "        -4.5574e-03, -3.8019e-01,  2.9097e-01,  9.8815e-02, -1.0495e-01,\n",
      "        -5.6000e-02,  1.5482e-01,  1.1995e-01,  3.3249e-02,  1.2599e-01,\n",
      "        -2.6799e-01, -7.7074e-02,  8.8924e-02, -6.3612e-02, -1.0585e-01,\n",
      "         1.2324e-01,  4.0492e-02, -2.8839e-01, -3.2468e-01,  5.3983e-02,\n",
      "         3.4860e-02,  2.5040e-01, -3.0255e-03,  2.2337e-01,  3.1907e-01,\n",
      "        -2.1658e-02, -1.8557e-01,  2.1477e-01, -1.1404e-01,  7.4089e-01,\n",
      "        -6.1433e-01, -1.2295e-01, -9.5836e-02,  4.3823e-01, -3.7980e-02,\n",
      "         1.8167e-01, -2.6254e-02,  1.4263e-01,  3.6159e-01,  8.4078e-02,\n",
      "         1.1251e-01, -1.2322e-01,  1.3860e-01,  1.1225e-01, -2.8469e-01,\n",
      "         1.3463e-01, -6.6936e-02,  2.1461e-01, -1.5457e-01, -1.1107e-01,\n",
      "        -4.9561e-02,  6.8731e-02,  1.3537e-01,  2.1421e-01, -2.9992e-02,\n",
      "        -1.3843e-01, -3.3275e-02,  1.4941e-01, -5.8844e-03, -4.2670e-01,\n",
      "        -3.3670e-01, -2.6439e-01, -6.3894e-02, -2.1524e-01, -2.5002e-04,\n",
      "         2.2869e-02,  2.2171e-02, -3.0314e-01, -3.6378e-03,  3.9033e-02,\n",
      "         2.0416e-01, -4.2845e-01,  8.7421e-02, -3.7153e-01,  5.4738e-01,\n",
      "        -2.6182e-01,  2.6141e-02, -9.0658e-02, -3.3685e-01, -9.7873e-02,\n",
      "        -1.1169e-01,  5.0154e-02,  1.2532e-02, -1.8920e-01, -2.0265e-01,\n",
      "        -1.5658e-01,  3.8714e-01,  2.8548e-01, -3.7468e-01, -2.1935e-01,\n",
      "         1.2790e-01, -2.6484e-01,  1.0520e-02, -1.1949e-01, -1.0998e-01,\n",
      "        -8.0425e-01, -3.0311e-01,  5.8960e-02, -2.7491e-01,  1.1939e-01,\n",
      "         2.8589e-02,  1.0077e-01, -1.2393e-02,  3.2476e-01, -1.9296e-01,\n",
      "        -5.6216e-02, -1.2873e-01,  2.3945e-01, -3.6129e-01,  8.7360e-03,\n",
      "        -3.3748e-02,  1.0819e-01,  1.2352e+00, -2.6262e-01,  1.6292e-01,\n",
      "         1.3084e-01, -1.6342e-01, -1.7731e-01])\n"
     ]
    }
   ],
   "source": [
    "print(sentence_embedding1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da28062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġbank', 'Ġof', 'Ġriver', 'Ġis', 'Ġawesome', '.']\n",
      "Ġbank\n",
      "---------\n",
      "['The', 'Ġbank', 'Ġof', 'Ġind', 'ia', 'Ġis', 'Ġawesome', '.']\n",
      "Ġbank\n",
      "---------\n",
      "1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)\n",
    "print(tokens1[1])\n",
    "print(\"---------\")\n",
    "print(tokens2)\n",
    "print(tokens2[1])\n",
    "\n",
    "print(\"---------\")\n",
    "print(cosine_similarity(word_embeddings1[2], word_embeddings2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e92d48",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\"> A value slightly greater than 1 (like 1.0000001192092896) can occur due to floating-point precision errors in the calculations. This is common in numerical computations, especially when dealing with high-dimensional data or very small differences in vectors. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aae5f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'Ġbank', 'Ġof', 'Ġind', 'ia', 'Ġand', 'Ġbank', 'Ġof', 'Ġbar', 'oda', 'Ġsituated', 'Ġat', 'Ġthe', 'Ġbank', 'Ġof', 'Ġg', 'anges', 'Ġis', 'Ġawesome']\n",
      "Ġbank\n",
      "Ġbank\n",
      "Ġbank\n",
      "0.9868789911270142\n",
      "0.9937019348144531\n",
      "0.996721088886261\n"
     ]
    }
   ],
   "source": [
    "sentence5 = \"the bank of india and bank of baroda situated at the bank of ganges is awesome\"\n",
    "sentence_embedding5, word_embeddings5, tokens5 = get_sentence_embedding(sentence5)\n",
    "\n",
    "print(tokens5)\n",
    "print(tokens5[1])\n",
    "print(tokens5[6])\n",
    "print(tokens5[13])\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[7]))\n",
    "print(cosine_similarity(word_embeddings5[2], word_embeddings5[14]))\n",
    "print(cosine_similarity(word_embeddings5[7], word_embeddings5[14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb234ed3",
   "metadata": {},
   "source": [
    "<div style=\"color:#722F37;\">\n",
    "<strong>BERT</strong> tends to perform better than <strong>RoBERTa</strong> and <strong>GPT-2</strong> in tasks involving <strong>word sense disambiguation</strong> and <strong>nuanced language understanding</strong> for a few reasons. First, BERT is a <strong>bidirectional transformer</strong> that processes text by looking at both the left and right context of each word, which allows it to capture the full context of a word within a sentence. This is particularly useful when disambiguating <strong>polysemous words</strong> (e.g., \"bank\" meaning a financial institution vs. a riverbank), as BERT uses context from all surrounding words to understand the meaning. In contrast, <strong>GPT-2</strong> is an <strong>autoregressive model</strong> that only predicts the next word based on preceding words, making it less effective at using the full sentence context to differentiate meanings. While <strong>RoBERTa</strong> also uses a bidirectional transformer like BERT, its training involves larger batches and longer sequences, but it lacks BERT's <strong>next sentence prediction (NSP)</strong> objective, which helps BERT model relationships between sentences more effectively. This NSP task may give BERT an edge in understanding complex sentence relationships, leading to better performance in certain nuanced language tasks.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b0fed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
